##Log_MedianIncome = log(median_income + 1),
# Assuming your dependent variable is named `house_value`
##) %>%
##select(
# Select both transformed and untransformed variables
##longitude,  # Untransformed variable
##Log_Latitude, Log_HousingMedianAge, Log_TotalRooms,
##Log_TotalBedrooms, Log_Population, Log_Households, Log_MedianIncome,
##median_house_value, ocean_proximity  # Ensure this matches the actual column name exactly
##)
##log_linear_model <- lm(median_house_value ~ longitude+Log_Latitude+Log_HousingMedianAge+
##Log_TotalRooms+Log_TotalBedrooms+Log_Population+Log_Households+
##Log_MedianIncome+ocean_proximity,data =data_1)
##hist(log_linear_model$residuals, main = "Histogram of Residuals", xlab = "Residuals")
# Load the necessary libraries
##library(ggplot2)
#install.packages("gridExtra")
##library(gridExtra)
# Assuming your residuals are stored in a variable called residuals
##residuals <- log_linear_model$residuals
# Create a data frame from the residuals
##df <- data.frame(Residuals = residuals)
# Histogram with a bell curve
##hist_plot <- ggplot(df, aes(x = Residuals)) +
##geom_histogram(aes(y = ..density..), bins = 30, fill = "grey", color = "black") +
##stat_function(fun = dnorm, args = list(mean = mean(df$Residuals), sd = sd(df$Residuals)),
##color = "black", size = 1) +
##ggtitle("Histogram of Residuals") +
##xlab("Residuals") +
##ylab("Density")
# Q-Q plot
##qq_plot <- ggplot(df, aes(sample = Residuals)) +
##stat_qq() +
##stat_qq_line() +
##ggtitle("Q-Q Plot of Residuals") +
##xlab("Theoretical Quantiles") +
##ylab("Sample Quantiles")
# Combine plots
##grid.arrange(hist_plot, qq_plot, nrow = 1)
##summary(log_linear_model)
# Calculate residuals for the training data
#train_residuals <- residuals(linear_model) #(ya b thk)
##train_residuals <- log_linear_model$residuals
# Create a data frame with residuals and corresponding ocean_proximity
##residuals_data <- data.frame(Residuals = train_residuals, Ocean_Proximity = train_data_1$ocean_proximity)
##residuals_data
# Create a boxplot of residuals before removing outliers
##library(ggplot2)
##ggplot(residuals_data, aes(y = Residuals, fill = Ocean_Proximity)) +
##geom_boxplot() +
##labs(title = "Boxplot of Residuals Before Removing Outliers", y = "Residuals", fill = "Ocean Proximity") +
##theme_minimal()
# Identify and remove outliers
##q1 <- quantile(train_residuals, 0.25)
##q3 <- quantile(train_residuals, 0.75)
##iqr <- q3 - q1
##lower_bound <- q1 - 1.5 * iqr
##upper_bound <- q3 + 1.5 * iqr
# Filter out the outliers in the training data
##outliers_filtered <- train_data_1[!(train_residuals < lower_bound | train_residuals > upper_bound), ]
##outliers_filtered_linear_model <- lm(median_house_value ~ longitude+Log_Latitude+Log_HousingMedianAge+
##Log_TotalRooms+Log_TotalBedrooms+Log_Population+Log_Households+
##Log_MedianIncome+ocean_proximity,data =outliers_filtered)
##summary_model <- summary(outliers_filtered_linear_model)
##adj_r2 <- summary_model$adj.r.squared
##adj_r2
# Calculate residuals using the filtered training data
##train_residuals_filtered <- residuals(outliers_filtered_linear_model)
# Create a data frame with residuals and corresponding predictor variable
##residuals_data_filtered <- data.frame(Residuals = train_residuals_filtered, Ocean_Proximity = outliers_filtered$ocean_proximity)
# Plot the boxplot of residuals after removing outliers
##library(ggplot2)
##ggplot(residuals_data_filtered, aes(y = Residuals, fill = Ocean_Proximity)) +
##geom_boxplot() +
##labs(title = "Boxplot of Residuals After Removing Outliers", y = "Residuals", fill = "Ocean Proximity") +
##theme_minimal()
residuals <- linear_model$residuals
# Create a data frame with residuals and corresponding ocean_proximity
residuals_data <- data.frame(Residuals = residuals, Ocean_Proximity = data$ocean_proximity)
#residuals_data
# Create a boxplot of residuals before removing outliers
library(ggplot2)
ggplot(residuals_data, aes(y = Residuals, fill = Ocean_Proximity)) +
geom_boxplot() +
labs(title = "Boxplot of Residuals Before Removing Outliers", y = "Residuals", fill = "Ocean Proximity") +
theme_minimal()
# Identify and remove outliers
q1 <- quantile(residuals, 0.25)
q3 <- quantile(residuals, 0.75)
iqr <- q3 - q1
lower_bound <- q1 - 1.5 * iqr
upper_bound <- q3 + 1.5 * iqr
# Filter out the outliers in the training data
outliers_filtered <- data[!(residuals < lower_bound | residuals > upper_bound), ]
# Identify and remove outliers
q1 <- quantile(residuals, 0.25)
q3 <- quantile(residuals, 0.75)
iqr <- q3 - q1
lower_bound <- q1 - 1.5 * iqr
upper_bound <- q3 + 1.5 * iqr
# Filter out the outliers in the training data
outliers_filtered <- data[!(residuals < lower_bound | residuals > upper_bound), ]
# Fit the linear model on the filtered training data
outliers_filtered_linear_model <- lm(median_house_value ~ ., data = outliers_filtered)
# extract Adj R2
summary_model <- summary(outliers_filtered_linear_model)
adj_r2 <- summary_model$adj.r.squared
adj_r2
# Calculate residuals using the filtered training data
residuals_filtered <- residuals(outliers_filtered_linear_model)
# Create a data frame with residuals and corresponding predictor variable
residuals_data_filtered <- data.frame(Residuals = residuals_filtered, Ocean_Proximity = outliers_filtered$ocean_proximity)
# Plot the boxplot of residuals after removing outliers
library(ggplot2)
p<- ggplot(residuals_data_filtered, aes(y = Residuals, fill = Ocean_Proximity)) +
geom_boxplot() +
labs(title = "Boxplot of Residuals After Removing Outliers", y = "Residuals", fill = "Ocean Proximity") +
theme_minimal()
library(plotly)
ggplotly(p)
# # Load necessary libraries
# # library(ggplot2)
#
# # Fit the initial multiple linear regression model
# linear_model <- lm(median_house_value ~ ., data = train_data)
#
# # Calculate the residuals
# train_residuals <- residuals(linear_model)
#
# # Create a data frame with residuals and corresponding ocean_proximity
# residuals_data <- data.frame(Residuals = train_residuals, Ocean_Proximity = train_data$ocean_proximity)
#
# # Create a boxplot of residuals before removing outliers
# ggplot(residuals_data, aes(y = Residuals, fill = Ocean_Proximity)) +
#   geom_boxplot() +
#   labs(title = "Boxplot of Residuals Before Removing Outliers", y = "Residuals", fill = "Ocean Proximity") +
#   theme_minimal()
#
# # ### Remove Outliers Using Z-Score Method
#
# # Calculate Z-scores for the residuals
# z_scores <- scale(train_residuals)
#
# # Set the threshold for identifying outliers (e.g., Z-score > 3 or < -3)
# threshold <- 3
#
# # Identify outliers
# outliers <- abs(z_scores) > threshold
#
# # Filter out the outliers in the training data
# outliers_filtered <- train_data[!outliers, ]
#
# # Fit the linear model on the filtered training data
# outliers_filtered_linear_model <- lm(median_house_value ~ ., data = outliers_filtered)
#
# # Extract Adjusted R-squared
# summary_model <- summary(outliers_filtered_linear_model)
# adj_r2 <- summary_model$adj.r.squared
# adj_r2
#
# # Calculate residuals using the filtered training data
# train_residuals_filtered <- residuals(outliers_filtered_linear_model)
#
# # Create a data frame with residuals and corresponding predictor variable
# residuals_data_filtered <- data.frame(Residuals = train_residuals_filtered, Ocean_Proximity = outliers_filtered$ocean_proximity)
#
# # Plot the boxplot of residuals after removing outliers
# p <- ggplot(residuals_data_filtered, aes(y = Residuals, fill = Ocean_Proximity)) +
#   geom_boxplot() +
#   labs(title = "Boxplot of Residuals After Removing Outliers", y = "Residuals", fill = "Ocean Proximity") +
#   theme_minimal()
# library(plotly)
# ggplotly(p)
# Check normality of the residuals using Anderson-Darling test
#library(nortest)
#ad_test <- ad.test(train_residuals_filtered)
#print(ad_test)
# Create Q-Q plot of the residuals after removing outliers
#qq_plot <- ggplot(data.frame(Residuals = train_residuals_filtered), aes(sample = Residuals)) +
#stat_qq() +
#stat_qq_line() +
#ggtitle("Q-Q Plot of Residuals After Removing Outliers") +
#xlab("Theoretical Quantiles") +
# ylab("Sample Quantiles")
#print(qq_plot)
# residuals_data_filtered <- residuals_data_filtered %>%
#   filter(!(Ocean_Proximity == "INLAND" & (Residuals < -104691.96 | Residuals > 102637.64))) %>%
#   filter(!(Ocean_Proximity == "<1H OCEAN" & (Residuals < -147994.07 | Residuals > 139754.52)))
#
# p<-ggplot(residuals_data_filtered, aes(y = Residuals, fill = Ocean_Proximity)) +
#   geom_boxplot() +
#   labs(title = "Boxplot of Residuals After Removing Outliers", y = "Residuals", fill = "Ocean Proximity") +
#   theme_minimal()
# ggplotly(p)
# Graphical Test: Residuals vs. Fitted plot
plot(linear_model$fitted.values, linear_model$residuals, main = "Residuals vs. Fitted", xlab = "Fitted Values", ylab = "Residuals",col="orange")
abline(h = 0, col = "black", lty = 2)
library(lmtest)
bptest(linear_model)
# Load necessary libraries
library(ggplot2)
# Generate a sample dataset with homoscedastic residuals
set.seed(123)  # For reproducibility
n <- 100
x <- rnorm(n, mean = 0, sd = 1)
y <- 3 * x + rnorm(n, mean = 0, sd = 1)  # Linear relationship with constant variance
# Fit a linear model
model <- lm(y ~ x)
# Create a data frame with fitted values and residuals
data_2 <- data.frame(
Fitted = fitted(model),
Residuals = residuals(model)
)
# Plot the residuals vs. fitted values
ggplot(data_2, aes(x = Fitted, y = Residuals)) +
geom_point() +
geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
geom_smooth(method = "lm", se = FALSE, color = "green", linetype = "dotted") +
labs(title = "Residuals vs. Fitted Values (Homoscedasticity)", x = "Fitted Values", y = "Residuals") +
theme_minimal()
# # Fit the linear regression model using ordinary least squares (OLS)
# linear_model_ols <- lm(median_house_value ~ ., data = train_data)
#
# # Calculate the residuals from the OLS model
# residuals_ols <- residuals(linear_model_ols)
#
# # Calculate the absolute residuals
# absolute_residuals <- abs(residuals_ols)
#
# # Calculate weights based on the absolute residuals
# weights <- 1 / absolute_residuals
#
# # Fit the weighted least squares (WLS) model using the weights
# linear_model_wls <- lm(median_house_value ~ ., data = data, weights = weights)
#
# # Check the summary of the WLS model
# summary(linear_model_wls)
#
# # Graphical Test: Residuals vs. Fitted plot
# plot(linear_model_wls$fitted.values, linear_model_wls$residuals, main = "Residuals vs. Fitted", xlab = "Fitted Values", ylab = "Residuals",col="orange")
# abline(h = 0, col = "black", lty = 2)
# library(lmtest)
# bptest(linear_model_wls)
# Graphical Test: Time series plot of residuals
plot(linear_model$residuals, type = "l", main = "Time Series Plot of Residuals",
col="green")
abline(h = 0, col = "blue", lty = 2)
library(zoo)
dwtest(linear_model)
# library(GGally) # corr plot
#ggscatmat(auto,columns =1:ncol(auto)) # do this if you have more comp power
library(mctest) # for multicollinearity test
install.packages("mctest")
library(mctest) # for multicollinearity test
mctest(linear_model, type="i")
# install.packages("mctest")
library(mctest) # for multicollinearity test
mctest(linear_model, type="i")
# Create component plus residual (CR) plots
library(car)
vif(linear_model)
# Assuming your residuals are stored in a variable called residuals
residuals <- linear_model$residuals
# Apply log transformation to the residuals
log_transformed_residuals <- log(residuals + 1)  # Adding 1 to handle zero or negative values
# Check normality of the log transformed residuals using Anderson-Darling test
ad_test_log_transformed <- ad.test(log_transformed_residuals)
print(ad_test_log_transformed)
# Create a Q-Q plot for the log transformed residuals
qq_plot_log_transformed <- ggplot(data.frame(Residuals = log_transformed_residuals), aes(sample = Residuals)) +
stat_qq() +
stat_qq_line() +
ggtitle("Q-Q Plot of Log Transformed Residuals") +
xlab("Theoretical Quantiles") +
ylab("Sample Quantiles")
print(qq_plot_log_transformed)
linear_model <- lm(median_house_value ~ ., data = data)
# Apply Yeo-Johnson transformation to the residuals
library(bestNormalize)
linear_model <- lm(median_house_value ~ ., data = data)
# Apply Yeo-Johnson transformation to the residuals
install.packages("bestNormalize")
library(bestNormalize)
yeo_johnson_result <- yeojohnson(residuals)
yeo_johnson_transformed_residuals <- yeo_johnson_result$x.t
# Check normality of the Yeo-Johnson transformed residuals using Anderson-Darling test
ad_test_yeo_johnson_transformed <- ad.test(yeo_johnson_transformed_residuals)
print(ad_test_yeo_johnson_transformed)
# Create a Q-Q plot for the Yeo-Johnson transformed residuals
qq_plot_yeo_johnson_transformed <- ggplot(data.frame(Residuals = yeo_johnson_transformed_residuals), aes(sample = Residuals)) +
stat_qq() +
stat_qq_line() +
ggtitle("Q-Q Plot of Yeo-Johnson Transformed Residuals") +
xlab("Theoretical Quantiles") +
ylab("Sample Quantiles")
print(qq_plot_yeo_johnson_transformed)
saveRDS(linear_model, "linear_model.rds")
# library(randomForest)
# random_forest_model <- randomForest(median_house_value ~ ., data = train_data, ntree = 500)
#
# # Predict on test data
# random_forest_predictions <- predict(random_forest_model, test_data)
#
# # Evaluate
# random_forest_evaluation <- evaluate_model(test_data$median_house_value, random_forest_predictions)
# print(random_forest_evaluation)
# # Retrieve variable names for categorical variables
# cat_var_names <- random_forest_model$forest$ncat
#
# # Retrieve variable names for continuous variables
# cont_var_names <- names(random_forest_model$forest$xlevels)
#
# # Combine variable names
# all_var_names <- c(cat_var_names, cont_var_names)
#
# # Print variable names
# print(all_var_names)
# Save the Loess model using serialization
#saveRDS(random_forest_model, "random_forest_model.rds")
# Load necessary libraries
# install.packages("caret")
library(caret)
# Load necessary libraries
install.packages("caret")
library(caret)
library(dplyr)
# Load the dataset (assuming train_data and test_data are already available)
# If not, load the dataset here
# train_data <- read.csv("train_data.csv")
# test_data <- read.csv("test_data.csv")
# Ensure all predictors are numeric
numeric_predictors <- select_if(data, is.numeric)
# Create polynomial features for a specified degree (e.g., degree = 2)
degree <- 2
poly_features <- as.data.frame(poly(as.matrix(numeric_predictors), degree = degree, raw = TRUE))
# Generate valid column names
colnames(poly_features) <- make.names(colnames(poly_features))
# Combine polynomial features with the target variable
data_poly <- cbind(poly_features, median_house_value = data$median_house_value)
# Fit the polynomial regression model
formula <- as.formula(paste("median_house_value ~", paste(colnames(poly_features), collapse = " + ")))
poly_model <- lm(formula, data = data_poly)
summary(poly_model)
# Load necessary libraries
# install.packages("caret")
library(caret)
library(dplyr)
# Load the dataset (assuming train_data and test_data are already available)
# If not, load the dataset here
# train_data <- read.csv("train_data.csv")
# test_data <- read.csv("test_data.csv")
# Ensure all predictors are numeric
numeric_predictors <- select_if(data, is.numeric)
# Create polynomial features for a specified degree (e.g., degree = 2)
degree <- 2
poly_features <- as.data.frame(poly(as.matrix(numeric_predictors), degree = degree, raw = TRUE))
# Generate valid column names
colnames(poly_features) <- make.names(colnames(poly_features))
# Combine polynomial features with the target variable
data_poly <- cbind(poly_features, median_house_value = data$median_house_value)
# Fit the polynomial regression model
formula <- as.formula(paste("median_house_value ~", paste(colnames(poly_features), collapse = " + ")))
poly_model <- lm(formula, data = data_poly)
summary(poly_model)
# Load necessary libraries
library(caret)
library(dplyr)
# Load the dataset (assuming train_data and test_data are already available)
# If not, load the dataset here
# train_data <- read.csv("train_data.csv")
# test_data <- read.csv("test_data.csv")
# Ensure all predictors are numeric
numeric_predictors <- select_if(data, is.numeric)
data2 <- data
# Assume 'median_house_value' is the target variable and we want to fit polynomial regression for 'median_income' as an example
# Create polynomial features manually
data2$median_income_squared <- data2$median_income^2
# Fit the polynomial regression model manually
poly_model <- lm(median_house_value ~ median_income + median_income_squared, data=data2)
# Predict on test data
poly_predictions <- predict(poly_model, data=data2)
# Define the evaluate_model function to compute evaluation metrics
evaluate_model <- function(actual, predicted) {
mse <- mean((actual - predicted)^2)
rmse <- sqrt(mse)
mae <- mean(abs(actual - predicted))
r2 <- 1 - sum((actual - predicted)^2) / sum((actual - mean(actual))^2)
return(list(MSE = mse, RMSE = rmse, MAE = mae, R2 = r2))
}
# Evaluate the model
poly_evaluation <- evaluate_model(data2$median_house_value, poly_predictions)
print(poly_evaluation) #could not we used all variables instead of median_income
# library(ggplot2)
#
# # Fit Loess model
# loess_model <- loess(median_house_value ~ ., data = train_data)
#
# # Predict on test data
# loess_predictions <- predict(loess_model, newdata = test_data)
#
# # Evaluate
# loess_evaluation <- evaluate_model(test_data$median_house_value, loess_predictions)
# print("Loess Evaluation:")
# print(loess_evaluation)
#
# # Plotting
# ggplot(test_data, aes(x = median_income, y = median_house_value)) +
#   geom_point(alpha = 0.3, color = "blue") +
#   geom_line(aes(y = loess_predictions), color = "red") +
#   labs(title = "Loess Regression",
#        x = "Median Income", y = "Median House Value")
# Save the Loess model using serialization
#saveRDS(loess_model, "loess_model.rds")
# Load the Loess model in the Shiny app
#loess_model <- readRDS("loess_model.rds")
# library(writexl)
# Assuming your dataset is in a variable called `housing`
#write_csv(train_data, "train_data.csv")
evaluate_model <- function(actual, predicted) {
mae <- mean(abs(actual - predicted))
mse <- mean((actual - predicted)^2)
rmse <- sqrt(mse)
r2 <- 1 - sum((actual - predicted)^2) / sum((actual - mean(actual))^2)
return(list(MAE = mae, MSE = mse, RMSE = rmse, R2 = r2))
}
# Train Random Forest model
#install.packages("randomForest")
library(randomForest)
random_forest_model <- randomForest(median_house_value ~ ., data = data, ntree = 500)
random_forest_predictions <- predict(random_forest_model, data)
random_forest_evaluation <- evaluate_model(data$median_house_value, random_forest_predictions)
print(random_forest_evaluation)
random_forest_predictions <- predict(random_forest_model, data)
random_forest_evaluation <- evaluate_model(data$median_house_value, random_forest_predictions)
print(random_forest_evaluation)
saveRDS(random_forest_model, "random_forest_model.rds")
library(shiny); runApp('app - Copy.R')
# install.packages("shiny")
# install.packages("randomForest")
library(shiny)
library(randomForest)
# Load the data
housing <- read.csv("housing.csv")
# Load the saved models
random_forest_model <- readRDS("random_forest_model.rds")
linear_model <- readRDS("linear_model.rds")
# Ensure the levels of the factor are the same as in the training data
housing$ocean_proximity <- factor(housing$ocean_proximity)
# UI
ui <- fluidPage(
titlePanel("California Housing Prices Analysis"),
sidebarLayout(
sidebarPanel(
selectInput("model_choice", "Select Model:",
choices = c("Random Forest", "Linear Regression")),
numericInput("longitude", "Longitude:", value = -122),
numericInput("latitude", "Latitude:", value = 37),
numericInput("housing_median_age", "Housing Median Age:", value = 29),
numericInput("total_rooms", "Total Rooms:", value = 2000),
numericInput("total_bedrooms", "Total Bedrooms:", value = 400),
numericInput("population", "Population:", value = 1500),
numericInput("households", "Households:", value = 500),
numericInput("median_income", "Median Income:", value = 3.5),
selectInput("ocean_proximity", "Ocean Proximity:",
choices = levels(housing$ocean_proximity)),
actionButton("predict_button", "Predict")
),
mainPanel(
plotOutput("model_plot"),
textOutput("prediction_output")
)
)
)
# Server
server <- function(input, output) {
# Prediction
output$prediction_output <- renderText({
req(input$predict_button)
isolate({
# Prepare input data
input_data <- data.frame(
longitude = input$longitude,
latitude = input$latitude,
housing_median_age = input$housing_median_age,
total_rooms = input$total_rooms,
total_bedrooms = input$total_bedrooms,
population = input$population,
households = input$households,
median_income = input$median_income,
ocean_proximity = factor(input$ocean_proximity, levels = levels(housing$ocean_proximity))
)
# Make prediction based on selected model
prediction <- if(input$model_choice == "Random Forest") {
predict(random_forest_model, newdata = input_data)
} else {
predict(linear_model, newdata = input_data)
}
# Display prediction
paste("The predicted median house value is $", round(prediction, 2), ".")
})
})
# Plot output
output$model_plot <- renderPlot({
# For illustration purposes, you can plot any relevant data here
# For example, a scatter plot of latitude vs. longitude
plot(housing$longitude, housing$latitude, pch = 20, col = "blue",
xlab = "Longitude", ylab = "Latitude", main = "California Housing Prices")
grid()
})
}
# Run the application
shinyApp(ui = ui, server = server)
